import re
import json
import os
import glob
from collections import defaultdict
from sqllineage.utils.constant import LineageLevel
from sqllineage.runner import LineageRunner
from sqllineage.utils.helpers import split
from sqllineage.core.metadata.dummy import DummyMetaDataProvider

from src.zero_copy_metadata_service import get_metadata, is_service_running, is_metadata_loaded, get_service_status


# ============= ç®€å•å…¨å±€ç¼“å­˜æ–¹æ¡ˆ =============

# å…¨å±€å˜é‡ï¼šè¿›ç¨‹å†…å”¯ä¸€çš„å…ƒæ•°æ®æä¾›å™¨
_global_metadata_provider = None

def get_metadata_for_lineage(metadata_file_name: str = None):
    """
    è·å–è¡€ç¼˜åˆ†æç”¨çš„å…ƒæ•°æ®ï¼ˆç®€å•å…¨å±€ç¼“å­˜ç‰ˆæœ¬ï¼‰
    
    å®ç°æ–¹å¼ï¼š
    - ä½¿ç”¨å…¨å±€å˜é‡ç¼“å­˜ DummyMetaDataProvider å®ä¾‹
    - æ¯ä¸ªè¿›ç¨‹åªåˆ›å»ºä¸€æ¬¡ï¼Œåç»­ç›´æ¥è¿”å›
    - ä»£ç ç®€æ´ï¼Œæ€§èƒ½ä¼˜ç§€
    
    Args:
        metadata_file_name: å…ƒæ•°æ®æ–‡ä»¶åç§°ï¼ˆä¸å«åç¼€ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼
    
    Returns:
        DummyMetaDataProvider: å…ƒæ•°æ®æä¾›å™¨ï¼Œå¦‚æœæ²¡æœ‰å…ƒæ•°æ®åˆ™è¿”å›None
    """
    global _global_metadata_provider
    
    # ç¡®å®šä½¿ç”¨çš„å…ƒæ•°æ®æ–‡ä»¶å
    if metadata_file_name is None:
        return None
    
    # å¦‚æœå·²ç»ç¼“å­˜è¿‡ä¸”ä½¿ç”¨çš„æ˜¯ç›¸åŒæ–‡ä»¶ï¼Œç›´æ¥è¿”å›
    if _global_metadata_provider is not None:
        return _global_metadata_provider
    
    # ç¬¬ä¸€æ¬¡è°ƒç”¨æˆ–æ–‡ä»¶åå˜åŒ–ï¼Œéœ€è¦åˆå§‹åŒ–
    try:
        # æ£€æŸ¥é›¶æ‹·è´æœåŠ¡çŠ¶æ€
        if not is_metadata_loaded(metadata_file_name):
            print(f"âš ï¸  å…ƒæ•°æ®æœåŠ¡æœªè¿è¡Œæˆ–æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file_name}")
            return None
                
        # ä»é›¶æ‹·è´å…±äº«å†…å­˜è·å–å…ƒæ•°æ®
        metadata_dict = get_metadata(metadata_file_name)
        
        if metadata_dict:
            # åˆ›å»ºå¹¶ç¼“å­˜åˆ°å…¨å±€å˜é‡
            _global_metadata_provider = DummyMetaDataProvider(metadata_dict)
            
            # table_count = len(metadata_dict)
            # column_count = sum(len(cols) for cols in metadata_dict.values())
            # print(f"âœ… å…ƒæ•°æ®æä¾›å™¨åˆå§‹åŒ–æˆåŠŸ!")
            # print(f"   ğŸ“Š æ–‡ä»¶: {metadata_file_name}")
            # print(f"   ğŸ“Š è¡¨æ•°é‡: {table_count} ä¸ª")
            # print(f"   ğŸ“Š å­—æ®µæ•°é‡: {column_count} ä¸ª")
            # print(f"   ğŸ’¾ å·²ç¼“å­˜åˆ°è¿›ç¨‹å†…å­˜")
            
            return _global_metadata_provider
        else:
            print(f"âš ï¸  é›¶æ‹·è´æœåŠ¡è¿”å›ç©ºæ•°æ®: {metadata_file_name}")
            return None
            
    except Exception as e:
        print(f"âŒ è·å–å…ƒæ•°æ®å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·æ£€æŸ¥é›¶æ‹·è´æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
        return None


# è¡¨ç±»å‹æ ‡è®°å¸¸é‡
class TableTypeMarkers:
    """è¡¨ç±»å‹æ ‡è®°å¸¸é‡ï¼ˆä¾¿äºç¨‹åºå¤„ç†ï¼‰"""
    TEMP_TABLE_SUFFIX = "_TEMP_TBL"      # ä¸´æ—¶è¡¨æ ‡è®°åç¼€
    SUBQUERY_TABLE_SUFFIX = "_SUBQRY_TBL"  # å­æŸ¥è¯¢è¡¨æ ‡è®°åç¼€


def parse_etl_info_from_path(file_path, base_path):
    """
    ä»æ–‡ä»¶è·¯å¾„ä¸­è§£æETLä¿¡æ¯
    
    Args:
        file_path: å®Œæ•´æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ D:\aaa\hql\F-DD_00001\aaa.hql
        base_path: åŸºç¡€è·¯å¾„ï¼Œå¦‚ D:\aaa\hql
        
    Returns:
        dict: åŒ…å« etl_system, etl_job, appname çš„å­—å…¸
    """
    try:
        # æ ‡å‡†åŒ–è·¯å¾„ï¼ˆå¤„ç†è·¯å¾„åˆ†éš”ç¬¦ï¼‰
        file_path = os.path.normpath(file_path)
        base_path = os.path.normpath(base_path)

        # è·å–ç›¸å¯¹è·¯å¾„
        relative_path = os.path.relpath(file_path, base_path)

        # åˆ†å‰²è·¯å¾„ç»„ä»¶
        path_parts = relative_path.split(os.sep)

        if len(path_parts) >= 2:
            # etl_system = ç›®å½•åç§°
            etl_system = path_parts[0]

            # etl_job = æ–‡ä»¶åï¼ˆåŒ…å«æ‰©å±•åï¼‰
            etl_job = os.path.basename(file_path)

            # appname = etl_system æŒ‰ "_" åˆ†å‰²çš„å‰é¢éƒ¨åˆ†
            if '_' in etl_system:
                appname = etl_system.split('_')[0]
            else:
                appname = etl_system

            return {
                'etl_system': etl_system,
                'etl_job': etl_job,
                'appname': appname
            }
        else:
            # å¦‚æœè·¯å¾„ç»“æ„ä¸ç¬¦åˆé¢„æœŸï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºé»˜è®¤å€¼
            etl_job = os.path.basename(file_path)
            return {
                'etl_system': '',
                'etl_job': etl_job,
                'appname': ''
            }

    except Exception as e:
        print(f"è§£æè·¯å¾„å¤±è´¥: {e}")
        etl_job = os.path.basename(file_path) if file_path else ''
        return {
            'etl_system': '',
            'etl_job': etl_job,
            'appname': ''
        }


def extract_use_database(sql_statement):
    """
    ä»USEè¯­å¥ä¸­æå–æ•°æ®åº“åç§°
    
    Args:
        sql_statement: SQLè¯­å¥ï¼ˆå·²å¤„ç†è¿‡æ³¨é‡Šï¼‰
        
    Returns:
        str: æ•°æ®åº“åç§°ï¼Œå¦‚æœä¸æ˜¯USEè¯­å¥åˆ™è¿”å›None
    """
    if not sql_statement or not sql_statement.strip():
        return None

    # ç®€å•å¤„ç†ï¼Œç›´æ¥æ£€æŸ¥USEè¯­å¥
    sql_upper = sql_statement.strip().upper()
    words = sql_upper.split()

    if len(words) >= 2 and words[0] == 'USE':
        # æå–USEåé¢çš„æ•°æ®åº“åï¼Œå»æ‰å¯èƒ½çš„åˆ†å·å’Œå¼•å·
        db_name = words[1].rstrip(';').strip('`"[]')
        return db_name

    return None


def extract_temp_tables_from_script(sql_script):
    """
    ä»SQLè„šæœ¬ä¸­æå–ä¸´æ—¶è¡¨
    æ–°å®šä¹‰ï¼šæ‰€æœ‰CREATE TABLEçš„è¡¨éƒ½ç®—ä¸´æ—¶è¡¨
    """
    # æå–æ‰€æœ‰CREATE TABLEçš„è¡¨ï¼ˆåŒ…æ‹¬LOCAL/GLOBAL TEMPORARY TABLEï¼‰
    create_pattern = r'CREATE\s+(?:(?:LOCAL|GLOBAL)\s+)?(?:TEMPORARY\s+|TEMP\s+)?(?:TABLE|VIEW)\s+(?:IF\s+NOT\s+EXISTS\s+)?([^\s\(\;]+)'
    create_matches = re.findall(create_pattern, sql_script, re.IGNORECASE | re.MULTILINE)

    # æ¸…ç†è¡¨åï¼ˆå»æ‰å¼•å·ã€æ–¹æ‹¬å·ç­‰ï¼‰
    def clean_table_name(table_name):
        cleaned = table_name.strip('`"[]').lower()
        # å¦‚æœæœ‰æ•°æ®åº“å‰ç¼€ï¼Œåªä¿ç•™è¡¨åéƒ¨åˆ†
        if '.' in cleaned:
            cleaned = cleaned.split('.')[-1]
        return cleaned

    # æ‰€æœ‰CREATE TABLEçš„è¡¨éƒ½ç®—ä¸´æ—¶è¡¨
    temp_tables = {clean_table_name(table) for table in create_matches}

    print(f"æ£€æµ‹åˆ°çš„ä¸´æ—¶è¡¨ï¼ˆæ‰€æœ‰CREATE TABLEï¼‰: {temp_tables}")
    return temp_tables


def split_sql_statements(sql_script):
    """
    ä½¿ç”¨sqllineageçš„splitæ–¹æ³•æ¥æ­£ç¡®æ‹†åˆ†SQLè¯­å¥
    """
    try:
        statements = split(sql_script)
        return [stmt.strip() for stmt in statements if stmt.strip()]
    except Exception as e:
        print(f"ä½¿ç”¨sqllineageæ‹†åˆ†SQLå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ‹†åˆ†: {e}")
        statements = []
        parts = sql_script.split(';')
        for part in parts:
            part = part.strip()
            if part and not part.isspace():
                statements.append(part)
        return statements


def is_temp_table(table_identifier, temp_tables):
    """
    æ£€æŸ¥è¡¨æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    """
    if not table_identifier or not temp_tables:
        return False

    # æå–è¡¨åï¼ˆå¤„ç†å„ç§æ ¼å¼ï¼‰
    table_name = str(table_identifier).lower()
    if '.' in table_name:
        table_name = table_name.split('.')[-1]

    return table_name in temp_tables


def add_table_type_marker(table_name, is_temp_table, is_subquery_table):
    """
    ä¸ºè¡¨åæ·»åŠ ç±»å‹æ ‡è®°
    
    Args:
        table_name: åŸå§‹è¡¨å
        is_temp_table: æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
        is_subquery_table: æ˜¯å¦ä¸ºå­æŸ¥è¯¢è¡¨
    
    Returns:
        str: å¸¦æ ‡è®°çš„è¡¨å
    """
    if not table_name:
        return table_name

    # å¦‚æœæ˜¯å­æŸ¥è¯¢è¡¨ï¼Œä¼˜å…ˆæ ‡è®°ä¸ºå­æŸ¥è¯¢ï¼ˆå› ä¸ºå­æŸ¥è¯¢è¡¨ä¹Ÿå¯èƒ½æ˜¯ä¸´æ—¶çš„ï¼‰
    if is_subquery_table:
        return f"{table_name}{TableTypeMarkers.SUBQUERY_TABLE_SUFFIX}"
    elif is_temp_table:
        return f"{table_name}{TableTypeMarkers.TEMP_TABLE_SUFFIX}"
    else:
        return table_name


def create_unique_subquery_table_name(subquery_alias, etl_job, sql_no):
    """
    ğŸ¯ åˆ›å»ºå”¯ä¸€çš„å­æŸ¥è¯¢è¡¨å
    æ ¼å¼ï¼š{å­æŸ¥è¯¢åˆ«å}_{ETLä½œä¸šåå“ˆå¸Œ}_{SQLåºå·}_SUBQRY_TBL
    
    Args:
        subquery_alias: å­æŸ¥è¯¢åˆ«å
        etl_job: ETLä½œä¸šåç§°
        sql_no: SQLåºå·
        
    Returns:
        str: å”¯ä¸€çš„å­æŸ¥è¯¢è¡¨å
    """
    import hashlib

    # ä¸ºäº†é¿å…è¡¨åè¿‡é•¿ï¼Œä½¿ç”¨ETLä½œä¸šåçš„MD5å“ˆå¸Œå€¼å‰8ä½
    etl_hash = ""
    if etl_job:
        etl_hash = hashlib.md5(etl_job.encode('utf-8')).hexdigest()[:8]

    sql_no_str = str(sql_no) if sql_no is not None else "0"

    # æ„å»ºå”¯ä¸€è¡¨åï¼šåˆ«å_å“ˆå¸Œ_SQLåºå·_æ ‡è®°
    unique_table_name = f"{subquery_alias}_{etl_hash}_{sql_no_str}{TableTypeMarkers.SUBQUERY_TABLE_SUFFIX}"

    return unique_table_name


def extract_database_table_column(column_id, temp_tables, subquery_nodes, current_database='', etl_job='', sql_no=None):
    """
    ä»å­—æ®µIDä¸­æå–æ•°æ®åº“ã€è¡¨ã€å­—æ®µä¿¡æ¯ï¼Œå¹¶ä¸ºè¡¨åæ·»åŠ ç±»å‹æ ‡è®°
    ğŸ¯ å¢å¼ºç‰ˆï¼šä¸ºå­æŸ¥è¯¢è¡¨åæ·»åŠ è„šæœ¬å’ŒSQLåºå·ä¿¡æ¯ï¼Œç¡®ä¿å”¯ä¸€æ€§
    
    Args:
        column_id: å­—æ®µæ ‡è¯†ç¬¦
        temp_tables: ä¸´æ—¶è¡¨é›†åˆ
        subquery_nodes: å­æŸ¥è¯¢èŠ‚ç‚¹é›†åˆ
        current_database: å½“å‰é»˜è®¤æ•°æ®åº“ï¼ˆæ¥è‡ªUSEè¯­å¥ï¼‰
        etl_job: ETLä½œä¸šåç§°ï¼ˆç”¨äºå­æŸ¥è¯¢å”¯ä¸€æ€§ï¼‰
        sql_no: SQLåºå·ï¼ˆç”¨äºå­æŸ¥è¯¢å”¯ä¸€æ€§ï¼‰
    """
    if not column_id:
        return None

    parts = str(column_id).split('.')

    # æå–åŸºæœ¬ä¿¡æ¯
    if len(parts) >= 3:
        # database.table.column æ ¼å¼
        database = parts[0] if parts[0] not in ('<unknown>', '<default>') else ''
        table = parts[1]
        column = parts[2]
    elif len(parts) == 2:
        # table.column æ ¼å¼ï¼ˆæ— æ•°æ®åº“å‰ç¼€ï¼‰
        database = ''
        table = parts[0]
        column = parts[1]
    elif len(parts) == 1:
        # åªæœ‰å­—æ®µå
        database = ''
        table = ''
        column = parts[0]
    else:
        return None

    # åˆ¤æ–­è¡¨ç±»å‹å¹¶æ·»åŠ æ ‡è®°
    if table:
        is_temp = is_temp_table(table, temp_tables)
        is_subquery = table in subquery_nodes

        if is_subquery:
            # ğŸ¯ å…³é”®æ”¹è¿›ï¼šä¸ºå­æŸ¥è¯¢è¡¨åæ·»åŠ å”¯ä¸€æ€§æ ‡è¯†
            table_with_marker = create_unique_subquery_table_name(table, etl_job, sql_no)
            database = '<SUBQUERY_DB>'
            print(f"ğŸ”§ ä¸ºå­æŸ¥è¯¢è¡¨ {table} åˆ›å»ºå”¯ä¸€æ ‡è¯†: {table_with_marker}")
        else:
            table_with_marker = add_table_type_marker(table, is_temp, False)

            # å¤„ç†æ•°æ®åº“åç§°
            if is_temp and (not database or database == '<default>' or database == ''):
                # ä¸´æ—¶è¡¨æ²¡æœ‰æ•°æ®åº“åç§°æ—¶ï¼Œå¡«å……ä¸º SCHEMA_TMP
                database = 'SCHEMA_TMP'
                print(f"ğŸ”§ ä¸ºä¸´æ—¶è¡¨ {table} è®¾ç½®é»˜è®¤æ•°æ®åº“: SCHEMA_TMP")
            elif (not database or database == '<default>') and current_database:
                # éä¸´æ—¶è¡¨ä½¿ç”¨å½“å‰æ•°æ®åº“
                database = current_database
                print(f"ğŸ”§ ä¸ºè¡¨ {table} è¡¥å……é»˜è®¤æ•°æ®åº“: {current_database}")
    else:
        table_with_marker = table

    return {
        'database': database,
        'table': table_with_marker,
        'column': column
    }


def trace_lineage_through_subqueries(cytoscape_data, temp_tables, current_database=''):
    """
    åŸºäºsqllineageçš„SubQueryç±»å‹ä¿¡æ¯ï¼Œè¿½è¸ªè·¨å­æŸ¥è¯¢çš„è¡€ç¼˜å…³ç³»
    ä¿®æ”¹ï¼šä¸è¿‡æ»¤ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼Œè€Œæ˜¯ä¸ºå®ƒä»¬æ·»åŠ æ ‡è®°
    æ”¯æŒé»˜è®¤æ•°æ®åº“è¡¥å……
    ğŸ¯ æ–°å¢ï¼šè¿‡æ»¤è·¨è¶Šä¸­é—´èŠ‚ç‚¹çš„ç›´æ¥è¡€ç¼˜å…³ç³»ï¼Œåªä¿ç•™ç›¸é‚»èŠ‚ç‚¹é—´çš„å…³ç³»
    """
    # æ„å»ºèŠ‚ç‚¹å’Œè¾¹çš„æ˜ å°„
    nodes_dict = {}
    edges = []
    subquery_nodes = set()

    for item in cytoscape_data:
        data = item.get("data", {})
        item_id = data.get("id", "")

        if "source" in data and "target" in data:
            edges.append(data)
        else:
            nodes_dict[item_id] = data
            # è¯†åˆ«SubQueryèŠ‚ç‚¹
            if data.get("type") == "SubQuery":
                subquery_nodes.add(item_id)

    # æ„å»ºå›¾ç»“æ„ï¼šå‡ºè¾¹å’Œå…¥è¾¹æ˜ å°„
    outgoing_edges = defaultdict(list)  # node_id -> [target_ids]
    incoming_edges = defaultdict(list)  # node_id -> [source_ids]

    for edge in edges:
        source_id = edge.get("source", "")
        target_id = edge.get("target", "")
        if source_id and target_id:
            outgoing_edges[source_id].append(target_id)
            incoming_edges[target_id].append(source_id)

    # ğŸ¯ ä¼˜åŒ–ç‰ˆï¼šæ„å»ºç”¨äºæ£€æµ‹ä¸­é—´è·¯å¾„çš„è¾…åŠ©å‡½æ•°ï¼ˆBFSç‰ˆæœ¬ï¼‰
    def has_intermediate_path(source, target, max_depth=None):
        """
        BFSç‰ˆæœ¬ï¼šæ£€æµ‹sourceåˆ°targetä¹‹é—´æ˜¯å¦å­˜åœ¨ä¸­é—´è·¯å¾„ï¼ˆé•¿åº¦>1çš„è·¯å¾„ï¼‰
        
        ä¼˜åŠ¿ï¼š
        1. ä¸å—é€’å½’æ·±åº¦é™åˆ¶ï¼Œå¯ä»¥å¤„ç†ä»»æ„æ·±åº¦çš„åµŒå¥—
        2. ä½¿ç”¨BFSé¿å…é€’å½’æ ˆæº¢å‡º
        3. æœ‰æ•ˆé˜²æ­¢ç¯è·¯é—®é¢˜
        4. æ€§èƒ½æ›´ç¨³å®š
        
        Args:
            source: æºèŠ‚ç‚¹
            target: ç›®æ ‡èŠ‚ç‚¹
            max_depth: ä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼ˆå®é™…ä¸ä½¿ç”¨ï¼‰
            
        Returns:
            bool: æ˜¯å¦å­˜åœ¨ä¸­é—´è·¯å¾„
        """
        if source == target:
            return False

        visited = {source}
        queue = [(source, 0)]  # (èŠ‚ç‚¹, è·ç¦»)

        while queue:
            node, distance = queue.pop(0)

            for neighbor in outgoing_edges.get(node, []):
                if neighbor == target:
                    # æ‰¾åˆ°ç›®æ ‡ï¼Œæ£€æŸ¥è·¯å¾„é•¿åº¦
                    path_length = distance + 1
                    return path_length > 1  # é•¿åº¦>1è¡¨ç¤ºæœ‰ä¸­é—´è·¯å¾„
                elif neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor, distance + 1))

        return False  # æœªæ‰¾åˆ°è·¯å¾„

    # æ”¶é›†æ‰€æœ‰è¡€ç¼˜è·¯å¾„ï¼ˆåŒ…æ‹¬ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼‰
    lineage_paths = []

    def is_subquery_column(column_id):
        """åˆ¤æ–­å­—æ®µæ˜¯å¦å±äºSubQuery"""
        if not column_id or '.' not in column_id:
            return False
        table_part = column_id.split('.')[0]
        return table_part in subquery_nodes

    def is_temp_table_column(column_id):
        """åˆ¤æ–­å­—æ®µæ˜¯å¦å±äºä¸´æ—¶è¡¨"""
        if not column_id or '.' not in column_id:
            return False
        table_part = column_id.split('.')[0]
        return is_temp_table(table_part, temp_tables)

    def is_real_table_column(column_id):
        """åˆ¤æ–­å­—æ®µæ˜¯å¦å±äºçœŸå®è¡¨ï¼ˆéSubQueryä¸”éä¸´æ—¶è¡¨ï¼‰"""
        if not column_id:
            return False

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯SubQueryå­—æ®µ
        if is_subquery_column(column_id):
            return False

        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶è¡¨å­—æ®µ
        if is_temp_table_column(column_id):
            return False

        # æ£€æŸ¥è¯¥å­—æ®µçš„parent_candidates
        column_data = nodes_dict.get(column_id, {})
        parent_candidates = column_data.get("parent_candidates", [])

        for candidate in parent_candidates:
            if isinstance(candidate, dict):
                candidate_type = candidate.get("type", "")
                candidate_name = candidate.get("name", "")

                # å¦‚æœparentæ˜¯Tableç±»å‹ä¸”ä¸æ˜¯ä¸´æ—¶è¡¨ï¼Œåˆ™æ˜¯çœŸå®è¡¨å­—æ®µ
                if (candidate_type == "Table" and
                    not is_temp_table(candidate_name, temp_tables)):
                    return True

        # å¦‚æœæ²¡æœ‰parent_candidatesï¼Œé€šè¿‡å­—æ®µIDåˆ¤æ–­
        if '.' in column_id:
            table_part = column_id.split('.')[0]
            return not is_temp_table(table_part, temp_tables)

        return False

    def trace_through_temp_tables(start_column_id, visited=None):
        """è¿½è¸ªè·¨è¶Šä¸´æ—¶è¡¨çš„è¡€ç¼˜å…³ç³»ï¼Œè¿”å›æœ€ç»ˆçš„çœŸå®è¡¨å­—æ®µ"""
        if visited is None:
            visited = set()

        if start_column_id in visited:
            return []  # é¿å…å¾ªç¯

        visited.add(start_column_id)

        # å¦‚æœå½“å‰å­—æ®µå°±æ˜¯çœŸå®è¡¨å­—æ®µï¼Œç›´æ¥è¿”å›
        if is_real_table_column(start_column_id):
            return [start_column_id]

        # å¦‚æœæ˜¯ä¸´æ—¶è¡¨å­—æ®µæˆ–SubQueryå­—æ®µï¼Œç»§ç»­è¿½è¸ªå…¶æºå­—æ®µ
        real_sources = []
        for source_id in incoming_edges.get(start_column_id, []):
            deeper_sources = trace_through_temp_tables(source_id, visited.copy())
            real_sources.extend(deeper_sources)

        return real_sources

    def trace_to_real_source(subquery_column_id, visited=None):
        """ä»å­æŸ¥è¯¢å­—æ®µè¿½è¸ªåˆ°çœŸå®æºè¡¨å­—æ®µ"""
        if visited is None:
            visited = set()

        if subquery_column_id in visited:
            return []  # é¿å…å¾ªç¯

        visited.add(subquery_column_id)
        real_sources = []

        # æŸ¥æ‰¾è¯¥å­æŸ¥è¯¢å­—æ®µçš„æ‰€æœ‰æºå­—æ®µ
        for source_id in incoming_edges.get(subquery_column_id, []):
            if is_subquery_column(source_id):
                # å¦‚æœæºè¿˜æ˜¯å­æŸ¥è¯¢å­—æ®µï¼Œç»§ç»­é€’å½’è¿½è¸ª
                deeper_sources = trace_to_real_source(source_id, visited.copy())
                real_sources.extend(deeper_sources)
            elif is_real_table_column(source_id):
                # å¦‚æœæºæ˜¯çœŸå®è¡¨å­—æ®µï¼Œæ·»åŠ åˆ°ç»“æœ
                real_sources.append(source_id)
            elif is_temp_table_column(source_id):
                # å¦‚æœæºæ˜¯ä¸´æ—¶è¡¨å­—æ®µï¼Œè¿½è¸ªåˆ°çœŸå®è¡¨
                deeper_sources = trace_through_temp_tables(source_id, visited.copy())
                real_sources.extend(deeper_sources)

        return real_sources

    # å¤„ç†ç­–ç•¥ï¼šæ”¶é›†æ‰€æœ‰è¡€ç¼˜å…³ç³»ï¼ŒåŒ…æ‹¬æ¶‰åŠä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨çš„

    # ğŸ¯ ä¿®æ”¹ï¼šåªæ”¶é›†ç›¸é‚»èŠ‚ç‚¹é—´çš„ç›´æ¥è¡€ç¼˜å…³ç³»ï¼Œè¿‡æ»¤æ‰è·¨è¶Šä¸­é—´èŠ‚ç‚¹çš„å…³ç³»
    print("ğŸ” å¼€å§‹è¿‡æ»¤è·¨è¶Šä¸­é—´èŠ‚ç‚¹çš„è¡€ç¼˜å…³ç³»...")
    filtered_edges_count = 0
    total_edges_count = 0

    for edge in edges:
        source_id = edge.get("source", "")
        target_id = edge.get("target", "")

        # åªå¤„ç†å­—æ®µåˆ°å­—æ®µçš„è¾¹
        if source_id and target_id and '.' in source_id and '.' in target_id:
            total_edges_count += 1

            # ğŸ¯ å…³é”®æ”¹è¿›ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸­é—´è·¯å¾„
            if has_intermediate_path(source_id, target_id):
                filtered_edges_count += 1
                print(f"ğŸš« è¿‡æ»¤è·¨è¶Šå…³ç³»: {source_id} -> {target_id} (å­˜åœ¨ä¸­é—´è·¯å¾„)")
                continue  # è·³è¿‡è¿™ä¸ªç›´æ¥è¾¹ï¼Œå› ä¸ºå­˜åœ¨ä¸­é—´è·¯å¾„

            # å¦‚æœæ²¡æœ‰ä¸­é—´è·¯å¾„ï¼Œä¿ç•™è¿™ä¸ªç›´æ¥è¾¹
            lineage_paths.append({
                'source': source_id,
                'target': target_id
            })

    print(f"âœ… è¿‡æ»¤å®Œæˆï¼šæ€»è¾¹æ•°={total_edges_count}, è¿‡æ»¤æ‰={filtered_edges_count}, ä¿ç•™={total_edges_count - filtered_edges_count}")

    # ğŸ¯ æ–°å¢ï¼šå¯¹äºæœ‰å­æŸ¥è¯¢çš„æƒ…å†µï¼Œåœ¨è¿‡æ»¤ä¹‹åå¤„ç†å­æŸ¥è¯¢è¡€ç¼˜å…³ç³»
    if subquery_nodes:
        # å¤„ç†è·¨SubQueryçš„è¡€ç¼˜å…³ç³»ï¼Œä½†ä¸æ·»åŠ è·¨è¶Šå…³ç³»
        processed_subquery_columns = set()

        for edge in edges:
            source_id = edge.get("source", "")
            target_id = edge.get("target", "")

            # å¤„ç† SubQueryå­—æ®µ â†’ æœ€ç»ˆè¡¨å­—æ®µ çš„è¾¹
            if (is_subquery_column(source_id) and
                not is_subquery_column(target_id) and  # ç›®æ ‡ä¸æ˜¯å­æŸ¥è¯¢å­—æ®µ
                source_id not in processed_subquery_columns):

                processed_subquery_columns.add(source_id)

                # ğŸ”§ ä¿®æ”¹ï¼šä¸ç›´æ¥æ·»åŠ è·¨è¶Šå…³ç³»ï¼Œè€Œæ˜¯æ£€æŸ¥è¯¥è¾¹æ˜¯å¦å·²è¢«è¿‡æ»¤æœºåˆ¶å¤„ç†
                # å¦‚æœSubQueryå­—æ®µ->æœ€ç»ˆè¡¨å­—æ®µçš„è¾¹è¢«ä¿ç•™äº†ï¼Œè¯´æ˜å®ƒæ˜¯ç›¸é‚»å…³ç³»
                subquery_to_final_edge = {'source': source_id, 'target': target_id}
                if subquery_to_final_edge in lineage_paths:
                    # è¿™æ¡è¾¹å·²è¢«æ·»åŠ ä¸ºç›¸é‚»å…³ç³»ï¼Œè¯´æ˜æ²¡æœ‰è·¨è¶Šä¸­é—´èŠ‚ç‚¹
                    continue

                # å¦‚æœSubQueryå­—æ®µ->æœ€ç»ˆè¡¨å­—æ®µçš„è¾¹ä¸åœ¨å·²ä¿ç•™çš„å…³ç³»ä¸­ï¼Œ
                # è¯´æ˜å¯èƒ½æœ‰æ›´æ·±çš„åµŒå¥—ï¼Œéœ€è¦è¿½è¸ªåˆ°çœŸå®æºè¡¨
                real_sources = trace_to_real_source(source_id)

                for real_source_id in real_sources:
                    # ğŸ¯ å…³é”®ä¿®æ”¹ï¼šå¯¹è¿½è¸ªåˆ°çš„çœŸå®æºè¡¨å…³ç³»ä¹Ÿè¦è¿›è¡Œè¿‡æ»¤æ£€æŸ¥
                    if not has_intermediate_path(real_source_id, target_id):
                        lineage_paths.append({
                            'source': real_source_id,
                            'target': target_id
                        })
                    else:
                        print(f"ğŸš« è¿‡æ»¤è¿½è¸ªå…³ç³»: {real_source_id} -> {target_id} (å­˜åœ¨ä¸­é—´è·¯å¾„)")
    return lineage_paths, subquery_nodes


def process_cytoscape_lineage(cytoscape_data, temp_tables, current_database, etl_system, etl_job, sql_path, sql_no):
    """
    å¤„ç†cytoscapeæ ¼å¼çš„è¡€ç¼˜æ•°æ®
    ğŸ¯ å¢å¼ºç‰ˆï¼šæ”¯æŒå­æŸ¥è¯¢å”¯ä¸€æ€§æ ‡è¯†ï¼Œä¸è¿‡æ»¤ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼Œè€Œæ˜¯ä¸ºå®ƒä»¬æ·»åŠ æ ‡è®°
    æ”¯æŒé»˜è®¤æ•°æ®åº“è¡¥å……
    """
    lineage_records = []

    if not cytoscape_data:
        return lineage_records
    # ä½¿ç”¨åŸºäºsqllineage SubQueryç±»å‹çš„è¿½è¸ªç®—æ³•
    lineage_paths, subquery_nodes = trace_lineage_through_subqueries(cytoscape_data, temp_tables, current_database)

    for path in lineage_paths:
        source_id = path['source']
        target_id = path['target']

        # ğŸ¯ ä½¿ç”¨å¢å¼ºç‰ˆå‡½æ•°ï¼Œä¼ å…¥ETLä½œä¸šå’ŒSQLåºå·ä¿¡æ¯ä»¥ç¡®ä¿å­æŸ¥è¯¢å”¯ä¸€æ€§
        source_info = extract_database_table_column(source_id, temp_tables, subquery_nodes, current_database, etl_job, sql_no)
        if not source_info or not source_info['table']:
            continue

        # ğŸ¯ ä½¿ç”¨å¢å¼ºç‰ˆå‡½æ•°ï¼Œä¼ å…¥ETLä½œä¸šå’ŒSQLåºå·ä¿¡æ¯ä»¥ç¡®ä¿å­æŸ¥è¯¢å”¯ä¸€æ€§  
        target_info = extract_database_table_column(target_id, temp_tables, subquery_nodes, current_database, etl_job, sql_no)
        if not target_info or not target_info['table']:
            continue

        # æ·»åŠ è¡€ç¼˜è®°å½•ï¼ˆè¡¨åå·²åŒ…å«å”¯ä¸€æ€§æ ‡è¯†ï¼‰
        record = {
            'etl_system': etl_system,
            'etl_job': etl_job,
            'sql_path': sql_path,
            'sql_no': sql_no,
            'source_database': source_info['database'],
            'source_table': source_info['table'],  # å­æŸ¥è¯¢è¡¨å·²åŒ…å«å”¯ä¸€æ€§æ ‡è¯†
            'source_column': source_info['column'],
            'target_database': target_info['database'],
            'target_table': target_info['table'],  # å­æŸ¥è¯¢è¡¨å·²åŒ…å«å”¯ä¸€æ€§æ ‡è¯†
            'target_column': target_info['column']
        }

        lineage_records.append(record)

    print(f"âœ… è§£æå‡º {len(lineage_records)} æ¡å­—æ®µçº§è¡€ç¼˜å…³ç³»ï¼ˆå­æŸ¥è¯¢è¡¨åå·²æ·»åŠ å”¯ä¸€æ€§æ ‡è¯†ï¼‰")
    return lineage_records


# DDLå’Œæ§åˆ¶è¯­å¥ç±»å‹å¸¸é‡
class DDLStatementTypes:
    """ä¸éœ€è¦è§£æè¡€ç¼˜å…³ç³»çš„è¯­å¥ç±»å‹ï¼ˆç±»ä¼¼Javaé™æ€ç±»/æšä¸¾ï¼‰"""

    # ä¸éœ€è¦è§£æè¡€ç¼˜å…³ç³»çš„è¯­å¥å…³é”®å­—ï¼ˆæ³¨æ„ï¼šUSEè¯­å¥å·²ç§»é™¤ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
    SKIP_KEYWORDS = frozenset([
        'ALTER', 'DROP', 'GRANT', 'REVOKE', 'SET', 'SHOW',
        'DESCRIBE', 'DESC', 'EXPLAIN', 'ANALYZE', 'TRUNCATE',
        'COMMENT', 'REFRESH', 'MSCK', 'CACHE', 'UNCACHE',
        'CREATE DATABASE', 'CREATE SCHEMA', 'CREATE USER', 'CREATE ROLE',
        'CREATE INDEX', 'CREATE FUNCTION', 'CREATE PROCEDURE'
    ])


def is_ddl_or_control_statement(sql_statement):
    """
    æ£€æµ‹SQLè¯­å¥æ˜¯å¦ä¸ºä¸éœ€è¦è§£æè¡€ç¼˜å…³ç³»çš„è¯­å¥
    ä¿®æ”¹ï¼šUSEè¯­å¥ä¸è·³è¿‡ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†æ¥æå–æ•°æ®åº“å
    """
    if not sql_statement or not sql_statement.strip():
        return False, None

    # ç®€å•åˆ†è¯å¤„ç†ï¼ˆå‡è®¾è¾“å…¥å·²æ¸…ç†è¿‡æ³¨é‡Šï¼‰
    sql_upper = sql_statement.strip().upper()
    words = sql_upper.split()

    if not words:
        return False, None

    # ç‰¹æ®Šå¤„ç†USEè¯­å¥ - ä¸è·³è¿‡ï¼Œéœ€è¦æå–æ•°æ®åº“å
    if words[0] == 'USE':
        return False, None  # ä¸è·³è¿‡USEè¯­å¥ï¼Œè®©åç»­é€»è¾‘å¤„ç†

    # æ£€æŸ¥å•å…³é”®è¯è¯­å¥
    first_word = words[0]
    if first_word in DDLStatementTypes.SKIP_KEYWORDS:
        return True, first_word

    # æ£€æŸ¥ä¸¤å…³é”®è¯ç»„åˆè¯­å¥
    if len(words) >= 2:
        two_words = f"{words[0]} {words[1]}"
        if two_words in DDLStatementTypes.SKIP_KEYWORDS:
            return True, two_words

    # ç‰¹æ®Šå¤„ç†CREATEè¯­å¥
    if first_word == 'CREATE' and len(words) >= 2:
        second_word = words[1]

        # CREATE TABLE/VIEW è¯­å¥ç»†åˆ†
        if second_word in ('TABLE', 'VIEW') or (second_word in ('TEMPORARY', 'TEMP') and len(words) >= 3 and words[2] in ('TABLE', 'VIEW')):
            # å¦‚æœåŒ…å«ASå…³é”®å­—ï¼Œè¯´æ˜æ˜¯CREATE TABLE AS SELECTï¼Œæœ‰è¡€ç¼˜å…³ç³»ï¼Œéœ€è¦è§£æ
            if 'AS' in words and 'SELECT' in words:
                return False, None  # ä¸è·³è¿‡ï¼Œéœ€è¦è§£æè¡€ç¼˜å…³ç³»
            else:
                # çº¯CREATE TABLEå®šä¹‰è¯­å¥ï¼Œæ— è¡€ç¼˜å…³ç³»ï¼Œè·³è¿‡
                if second_word in ('TEMPORARY', 'TEMP'):
                    return True, f'CREATE {second_word} {words[2]}'
                else:
                    return True, f'CREATE {second_word}'

    # å…¶ä»–è¯­å¥ï¼ˆINSERTã€SELECTç­‰ï¼‰éƒ½ä¸è·³è¿‡ï¼Œæ­£å¸¸è§£æè¡€ç¼˜å…³ç³»
    return False, None


def is_from_statement(sql_statement):
    """
    æ£€æµ‹SQLè¯­å¥æ˜¯å¦ä»¥FROMå¼€å¤´ï¼ˆHiveç‰¹æ®Šè¯­æ³•ï¼‰
    æ”¯æŒå¤„ç†FROM(è¿™ç§è¿å†™çš„æƒ…å†µ
    
    Args:
        sql_statement: SQLè¯­å¥
        
    Returns:
        bool: æ˜¯å¦ä¸ºFROMå¼€å¤´çš„è¯­å¥
    """
    if not sql_statement or not sql_statement.strip():
        return False

    # å»é™¤å‰å¯¼ç©ºç™½å¹¶è½¬æ¢ä¸ºå¤§å†™
    sql_upper = sql_statement.strip().upper()

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…FROMå¼€å¤´ï¼ŒåŒ…æ‹¬FROM(çš„æƒ…å†µ
    from_pattern = r'^\s*FROM\s*(?:\(|\s)'
    return bool(re.match(from_pattern, sql_upper))


def process_single_sql(sql_statement, temp_tables, current_database, etl_system, etl_job, sql_path, sql_no, db_type='oracle'):
    """
    å¤„ç†å•æ¡SQLè¯­å¥ï¼Œè·å–è¡€ç¼˜å…³ç³»
    ä¿®æ”¹ï¼šæ”¯æŒUSEè¯­å¥å¤„ç†ã€é»˜è®¤æ•°æ®åº“ç»´æŠ¤å’ŒFROMå¼€å¤´è¯­å¥çš„non-validatingå¤„ç†ï¼Œæ”¯æŒå…ƒæ•°æ®
    
    Returns:
        tuple: (lineage_records, new_current_database)
    """
    lineage_records = []
    new_current_database = current_database

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºUSEè¯­å¥
    use_database = extract_use_database(sql_statement)
    if use_database:
        new_current_database = use_database
        return lineage_records, new_current_database

    # æ£€æŸ¥æ˜¯å¦ä¸ºDDLæˆ–æ§åˆ¶è¯­å¥
    is_ddl, stmt_type = is_ddl_or_control_statement(sql_statement)

    if is_ddl:
        print(f"â­ï¸  è·³è¿‡{stmt_type}è¯­å¥ï¼ˆæ— è¡€ç¼˜å…³ç³»è§£ææ„ä¹‰ï¼‰")
        return lineage_records, new_current_database

    # æ£€æŸ¥æ˜¯å¦ä¸ºFROMå¼€å¤´çš„è¯­å¥ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨non-validating dialect
    actual_db_type = db_type
    if is_from_statement(sql_statement):
        actual_db_type = 'non-validating'
        print(f"ğŸ”§ æ£€æµ‹åˆ°FROMå¼€å¤´è¯­å¥ï¼Œä½¿ç”¨non-validating dialectè§£æ")

    try:
        # ä½¿ç”¨LineageRunneråˆ†æSQLï¼Œæ ¹æ®è¯­å¥ç±»å‹é€‰æ‹©é€‚å½“çš„dialectï¼Œä¼ å…¥å…ƒæ•°æ®
        if _global_metadata_provider:
            runner = LineageRunner(sql_statement, dialect=actual_db_type, silent_mode=True, metadata_provider=_global_metadata_provider)
        else:
            runner = LineageRunner(sql_statement, dialect=actual_db_type, silent_mode=True)

        # è·å–cytoscapeæ ¼å¼çš„å­—æ®µçº§è¡€ç¼˜æ•°æ®
        try:
            cytoscape_data = runner.to_cytoscape(LineageLevel.COLUMN)

            if cytoscape_data and isinstance(cytoscape_data, list):
                lineage_records = process_cytoscape_lineage(cytoscape_data, temp_tables, current_database, etl_system, etl_job, sql_path, sql_no)
                print(f"âœ… è§£æå‡º {len(lineage_records)} æ¡å­—æ®µçº§è¡€ç¼˜å…³ç³»")
            else:
                print(f"âŒ {sql_path} æœªè·å–åˆ°å­—æ®µçº§è¡€ç¼˜æ•°æ®")

        except Exception as e:
            print(f"âŒ  {sql_path} è·å–å­—æ®µçº§è¡€ç¼˜å¤±è´¥: {e}")

    except Exception as e:
        print(f"âŒ  {sql_path} åˆ›å»ºLineageRunneræ—¶å‡ºé”™: {e}")

    return lineage_records, new_current_database


def generate_oracle_insert_statements(lineage_records, etl_system, etl_job):
    """
    ç”ŸæˆOracle INSERTè¯­å¥ï¼ˆåŒ…å«etl_systemã€etl_jobã€sql_pathã€sql_noå­—æ®µï¼‰
    ä¼˜åŒ–ç‰ˆï¼šç›´æ¥ä¼ å…¥etl_systemå’Œetl_jobå‚æ•°ï¼Œå…ˆåˆ é™¤å†æ‰¹é‡æ’å…¥ï¼Œæœ€åæäº¤äº‹åŠ¡
    
    Args:
        lineage_records: è¡€ç¼˜å…³ç³»è®°å½•åˆ—è¡¨
        etl_system: ETLç³»ç»Ÿåç§°
        etl_job: ETLä½œä¸šåç§°
        
    Returns:
        str: Oracle DELETEå’ŒINSERTè¯­å¥
    """
    insert_statements = []



    # ç”ŸæˆDELETEè¯­å¥
    insert_statements.append("-- ç¬¬ä¸€æ­¥ï¼šåˆ é™¤ç°æœ‰æ•°æ®ï¼ˆåŸºäºETL_SYSTEMå’ŒETL_JOBï¼‰")

    # etl_system å’Œ etl_job éƒ½ä¸ºç©ºæ—¶ï¼Œä¸åˆ é™¤æ•°æ®  é¿å…è¯¯åˆ æ•°æ®
    if etl_system != '' and etl_job != '' and etl_system != None and etl_job != None:
        delete_sql = f"DELETE FROM LINEAGE_TABLE WHERE ETL_SYSTEM = '{etl_system}' AND ETL_JOB = '{etl_job}';"
        insert_statements.append(delete_sql)
        #TODO æ‰§è¡Œæ•°æ®åº“åˆ é™¤æ“ä½œ

    insert_statements.append("")

    insert_statements.append("-- ç¬¬äºŒæ­¥ï¼šæ‰¹é‡æ’å…¥æ–°æ•°æ®")

    # ç”Ÿæˆæ‰€æœ‰INSERTè¯­å¥
    for record in lineage_records:
        def format_value(value):
            if not value or value == '':
                return 'NULL'
            else:
                escaped_value = str(value).replace("'", "''")
                return f"'{escaped_value}'"

        etl_system_val = format_value(record['etl_system'])
        etl_job_val = format_value(record['etl_job'])
        sql_path = format_value(record['sql_path'])
        sql_no = record['sql_no'] if record['sql_no'] is not None else 'NULL'
        source_db = format_value(record['source_database'])
        source_table = format_value(record['source_table'])
        source_column = format_value(record['source_column'])
        target_db = format_value(record['target_database'])
        target_table = format_value(record['target_table'])
        target_column = format_value(record['target_column'])

        insert_sql = f"""INSERT INTO LINEAGE_TABLE (ETL_SYSTEM, ETL_JOB, SQL_PATH, SQL_NO, SOURCE_DATABASE, SOURCE_TABLE, SOURCE_COLUMN, TARGET_DATABASE, TARGET_TABLE, TARGET_COLUMN)
VALUES ('{etl_system_val}, '{etl_job_val}', '{sql_path}', '{sql_no}', '{source_db}', '{source_table}', '{source_column}', '{target_db}', '{target_table}','{target_column}');"""

        insert_statements.append(insert_sql)

    insert_statements.append("")
    insert_statements.append("-- ç¬¬ä¸‰æ­¥ï¼šæäº¤äº‹åŠ¡")
    insert_statements.append("COMMIT;")
    #TODO å°†è„šæœ¬å†…æ‰€æœ‰sqlæ‹¼æ¥å¥½ä¹‹åï¼ŒæŒ‰ç…§è„šæœ¬ç»´åº¦æ‰§è¡Œ é¿å…é¢‘ç¹æäº¤äº‹åŠ¡

    return "\n".join(insert_statements)


def process_sql_script(sql_script, etl_system='', etl_job='', sql_path='', db_type=''):
    """
    å¤„ç†SQLè„šæœ¬ï¼ˆæ”¯æŒå•æ¡SQLæˆ–å®Œæ•´è„šæœ¬ï¼‰
    ä¿®æ”¹ï¼šæ”¯æŒUSEè¯­å¥å¤„ç†å’Œé»˜è®¤æ•°æ®åº“ç»´æŠ¤ï¼Œå…ˆåˆ é™¤å†æ’å…¥æ¨¡å¼ï¼Œæ”¯æŒå…ƒæ•°æ®
    """
    # 1. æå–ä¸´æ—¶è¡¨
    temp_tables = extract_temp_tables_from_script(sql_script)

    # 2. æ‹†åˆ†SQLè¯­å¥
    sql_statements = split_sql_statements(sql_script)
    print(f"å…±æ‰¾åˆ° {len(sql_statements)} æ¡SQLè¯­å¥")

    # 3. å¤„ç†æ¯æ¡SQLï¼Œç»´æŠ¤å½“å‰æ•°æ®åº“çŠ¶æ€
    all_lineage_records = []
    current_database = ''  # é»˜è®¤æ•°æ®åº“å˜é‡

    for i, sql in enumerate(sql_statements):
        sql_no = i + 1
        print(f"å¤„ç†ç¬¬ {sql_no}/{len(sql_statements)} æ¡SQL...")

        # å¤„ç†SQLè¯­å¥ï¼Œè¿”å›è¡€ç¼˜è®°å½•å’Œæ›´æ–°åçš„å½“å‰æ•°æ®åº“
        lineage_records, current_database = process_single_sql(
            sql, temp_tables, current_database, etl_system, etl_job, sql_path, sql_no, db_type
        )
        all_lineage_records.extend(lineage_records)

        print(f"  æ–°å¢ {len(lineage_records)} æ¡è¡€ç¼˜å…³ç³»")

    print(f"å…±æå–åˆ° {len(all_lineage_records)} æ¡è¡€ç¼˜å…³ç³»")

    # 4. ç”ŸæˆOracle DELETEå’ŒINSERTè¯­å¥
    oracle_statements = generate_oracle_insert_statements(all_lineage_records, etl_system, etl_job)

    return oracle_statements


def lineage_analysis(sql=None, file=None, db_type='oracle'):
    """
    è¡€ç¼˜å…³ç³»åˆ†æä¸»å…¥å£ï¼ˆé›¶æ‹·è´å…±äº«å†…å­˜å¢å¼ºç‰ˆï¼‰
    
    ğŸš€ æ–°ç‰¹æ€§ï¼š
    - é›†æˆé›¶æ‹·è´å…±äº«å†…å­˜å…ƒæ•°æ®æœåŠ¡ï¼Œæå¤§æå‡æ€§èƒ½
    - æ”¯æŒå¤šè¿›ç¨‹å¹¶å‘è®¿é—®ï¼Œæ— é”é«˜æ•ˆ
    - è‡ªåŠ¨é™çº§åˆ°ä¼ ç»ŸåŠ è½½å™¨ï¼Œç¡®ä¿å…¼å®¹æ€§
    
    Args:
        sql: SQLè„šæœ¬å†…å®¹å­—ç¬¦ä¸²
        file: SQLæ–‡ä»¶è·¯å¾„ï¼ˆå•ä¸ªæ–‡ä»¶æˆ–ç›®å½•ï¼‰
        db_type: æ•°æ®åº“ç±»å‹ï¼Œé»˜è®¤'oracle'
        
    Returns:
        str: Oracle DELETEå’ŒINSERTè¯­å¥ï¼ˆåŒ…å«æ ‡è®°çš„ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼Œæ”¯æŒé»˜è®¤æ•°æ®åº“ï¼Œæ”¯æŒé›¶æ‹·è´å…ƒæ•°æ®ï¼‰
    """

    """ åŠ è½½å…ƒæ•°æ®   """
    # å°è¯•åˆå§‹åŒ–å…ƒæ•°æ®æä¾›å™¨
    metadata_provider = get_metadata_for_lineage('metadata_config_template')
    if metadata_provider:
        print(f"âœ… å…ƒæ•°æ®åŠ è½½æˆåŠŸ")
    else:
        print(f"âš ï¸  æœªåŠ è½½å…ƒæ•°æ®ï¼Œå°†ä½¿ç”¨æ— å…ƒæ•°æ®æ¨¡å¼")

    if sql is not None and file is not None:
        raise ValueError("sqlå’Œfileå‚æ•°ä¸èƒ½åŒæ—¶æä¾›ï¼Œåªèƒ½é€‰æ‹©å…¶ä¸­ä¸€ä¸ª")

    if sql is None and file is None:
        raise ValueError("å¿…é¡»æä¾›sqlæˆ–fileå‚æ•°")

    if sql is not None:
        # å¤„ç†SQLå­—ç¬¦ä¸²
        return process_sql_script(sql, etl_system='DEMO_SYSTEM', etl_job='DEMO_JOB', sql_path='INLINE_SQL', db_type=db_type)

    elif file is not None:
        # å¤„ç†æ–‡ä»¶è·¯å¾„

        if os.path.isfile(file):
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    sql_content = f.read()

                # æ™ºèƒ½é€‰æ‹©base_pathï¼šå‘ä¸Šæ‰¾åˆ°åˆé€‚çš„åŸºç¡€ç›®å½•
                # ç­–ç•¥ï¼šå¦‚æœçˆ¶ç›®å½•çš„åç§°çœ‹èµ·æ¥åƒç³»ç»Ÿåï¼ˆåŒ…å«-æˆ–_ï¼‰ï¼Œåˆ™ä½¿ç”¨çˆ·çˆ·ç›®å½•ä½œä¸ºbase_path
                file_dir = os.path.dirname(file)
                parent_name = os.path.basename(file_dir)

                # å¦‚æœå½“å‰ç›®å½•ååŒ…å«å…¸å‹çš„ç³»ç»Ÿæ ‡è¯†ç¬¦ï¼Œä½¿ç”¨ä¸Šçº§ç›®å½•ä½œä¸ºbase_path
                if '-' in parent_name or '_' in parent_name:
                    base_path = os.path.dirname(file_dir)
                else:
                    base_path = file_dir

                etl_info = parse_etl_info_from_path(file, base_path)

                return process_sql_script(sql_content, etl_info['etl_system'], etl_info['etl_job'], file, db_type)

            except Exception as e:
                return f"-- å¤„ç†æ–‡ä»¶å¤±è´¥: {e}"

        elif os.path.isdir(file):
            # å¤„ç†ç›®å½• - ä½¿ç”¨æ–°çš„è·¯å¾„è§£æé€»è¾‘
            sql_extensions = ['*.sql', '*.SQL', '*.hql', '*.HQL']

            # ä½¿ç”¨é›†åˆå»é‡ï¼Œé¿å…Windowsç³»ç»Ÿä¸­å¤§å°å†™ä¸æ•æ„Ÿå¯¼è‡´çš„é‡å¤æ–‡ä»¶
            all_files = set()
            for ext in sql_extensions:
                pattern = os.path.join(file, '**', ext)
                files = glob.glob(pattern, recursive=True)
                all_files.update(files)

            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åºï¼Œç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
            sql_files = sorted(list(all_files))
            file_count = len(sql_files)

            if file_count == 0:
                return "-- æœªæ‰¾åˆ°ä»»ä½•SQLæ–‡ä»¶"

            print(f"æ‰¾åˆ° {file_count} ä¸ªSQLæ–‡ä»¶")

            all_results = []

            for i, sql_file in enumerate(sql_files):
                try:
                    print(f"\nå¤„ç†æ–‡ä»¶ {i+1}/{file_count}: {sql_file}")

                    with open(sql_file, 'r', encoding='utf-8') as f:
                        sql_content = f.read()

                    # ä»å®Œæ•´è·¯å¾„ä¸­è§£æETLä¿¡æ¯
                    etl_info = parse_etl_info_from_path(sql_file, file)
                    print(f"  è§£æåˆ°çš„ETLä¿¡æ¯: etl_system={etl_info['etl_system']}, etl_job={etl_info['etl_job']}, appname={etl_info['appname']}")

                    result = process_sql_script(sql_content, etl_info['etl_system'], etl_info['etl_job'], sql_file, db_type)
                    all_results.append(result)

                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶ {sql_file} å¤±è´¥: {e}")
                    all_results.append(f"-- æ–‡ä»¶: {sql_file}\n-- å¤„ç†å¤±è´¥: {e}")

            # åˆå¹¶ç»“æœ
            combined_result = []

            for result in all_results:
                combined_result.append(result)
                combined_result.append("")

            return "\n".join(combined_result)
        else:
            return f"-- è·¯å¾„ä¸å­˜åœ¨: {file}"


if __name__ == "__main__":

  

    # æµ‹è¯•SQLç¤ºä¾‹ï¼ˆåŒ…å«USEè¯­å¥ï¼‰
    test_sql = """
    use aam;
    insert into temp_customers
    SELECT customer_id, customer_name, email 
    FROM customers 
    WHERE status = 'active';
    """

    print("ğŸš€ å¼€å§‹SQLè¡€ç¼˜åˆ†ææµ‹è¯•...")
    result_with_metadata = lineage_analysis(sql=test_sql, db_type='oracle')
    print("\nğŸ“‹ åˆ†æç»“æœ:")
    print(result_with_metadata)

    
