import re
import json
import os
import glob
from collections import defaultdict
from sqllineage.utils.constant import LineageLevel
from sqllineage.runner import LineageRunner
from sqllineage.utils.helpers import split



# è¡¨ç±»å‹æ ‡è®°å¸¸é‡
class TableTypeMarkers:
    """è¡¨ç±»å‹æ ‡è®°å¸¸é‡ï¼ˆä¾¿äºç¨‹åºå¤„ç†ï¼‰"""
    TEMP_TABLE_SUFFIX = "_TEMP_TBL"      # ä¸´æ—¶è¡¨æ ‡è®°åç¼€
    SUBQUERY_TABLE_SUFFIX = "_SUBQRY_TBL"  # å­æŸ¥è¯¢è¡¨æ ‡è®°åç¼€


def parse_etl_info_from_path(file_path, base_path):
    """
    ä»æ–‡ä»¶è·¯å¾„ä¸­è§£æETLä¿¡æ¯
    
    Args:
        file_path: å®Œæ•´æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ D:\aaa\hql\F-DD_00001\aaa.hql
        base_path: åŸºç¡€è·¯å¾„ï¼Œå¦‚ D:\aaa\hql
        
    Returns:
        dict: åŒ…å« etl_system, etl_job, appname çš„å­—å…¸
    """
    try:
        # æ ‡å‡†åŒ–è·¯å¾„ï¼ˆå¤„ç†è·¯å¾„åˆ†éš”ç¬¦ï¼‰
        file_path = os.path.normpath(file_path)
        base_path = os.path.normpath(base_path)
        
        # è·å–ç›¸å¯¹è·¯å¾„
        relative_path = os.path.relpath(file_path, base_path)
        
        # åˆ†å‰²è·¯å¾„ç»„ä»¶
        path_parts = relative_path.split(os.sep)
        
        if len(path_parts) >= 2:
            # etl_system = ç›®å½•åç§°
            etl_system = path_parts[0]
            
            # etl_job = æ–‡ä»¶åï¼ˆåŒ…å«æ‰©å±•åï¼‰
            etl_job = os.path.basename(file_path)
            
            # appname = etl_system æŒ‰ "_" åˆ†å‰²çš„å‰é¢éƒ¨åˆ†
            if '_' in etl_system:
                appname = etl_system.split('_')[0]
            else:
                appname = etl_system
            
            return {
                'etl_system': etl_system,
                'etl_job': etl_job,
                'appname': appname
            }
        else:
            # å¦‚æœè·¯å¾„ç»“æ„ä¸ç¬¦åˆé¢„æœŸï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºé»˜è®¤å€¼
            etl_job = os.path.basename(file_path)
            return {
                'etl_system': '',
                'etl_job': etl_job,
                'appname': ''
            }
            
    except Exception as e:
        print(f"è§£æè·¯å¾„å¤±è´¥: {e}")
        etl_job = os.path.basename(file_path) if file_path else ''
        return {
            'etl_system': '',
            'etl_job': etl_job,
            'appname': ''
        }


def extract_use_database(sql_statement):
    """
    ä»USEè¯­å¥ä¸­æå–æ•°æ®åº“åç§°
    
    Args:
        sql_statement: SQLè¯­å¥ï¼ˆå·²å¤„ç†è¿‡æ³¨é‡Šï¼‰
        
    Returns:
        str: æ•°æ®åº“åç§°ï¼Œå¦‚æœä¸æ˜¯USEè¯­å¥åˆ™è¿”å›None
    """
    if not sql_statement or not sql_statement.strip():
        return None
    
    # ç®€å•å¤„ç†ï¼Œç›´æ¥æ£€æŸ¥USEè¯­å¥
    sql_upper = sql_statement.strip().upper()
    words = sql_upper.split()
    
    if len(words) >= 2 and words[0] == 'USE':
        # æå–USEåé¢çš„æ•°æ®åº“åï¼Œå»æ‰å¯èƒ½çš„åˆ†å·å’Œå¼•å·
        db_name = words[1].rstrip(';').strip('`"[]')
        return db_name
    
    return None


def extract_temp_tables_from_script(sql_script):
    """
    ä»SQLè„šæœ¬ä¸­æå–ä¸´æ—¶è¡¨
    æ–°å®šä¹‰ï¼šæ‰€æœ‰CREATE TABLEçš„è¡¨éƒ½ç®—ä¸´æ—¶è¡¨
    """
    # æå–æ‰€æœ‰CREATE TABLEçš„è¡¨
    create_pattern = r'CREATE\s+(?:TEMPORARY\s+|TEMP\s+)?(?:TABLE|VIEW)\s+(?:IF\s+NOT\s+EXISTS\s+)?([^\s\(\;]+)'
    create_matches = re.findall(create_pattern, sql_script, re.IGNORECASE | re.MULTILINE)

    # æ¸…ç†è¡¨åï¼ˆå»æ‰å¼•å·ã€æ–¹æ‹¬å·ç­‰ï¼‰
    def clean_table_name(table_name):
        cleaned = table_name.strip('`"[]').lower()
        # å¦‚æœæœ‰æ•°æ®åº“å‰ç¼€ï¼Œåªä¿ç•™è¡¨åéƒ¨åˆ†
        if '.' in cleaned:
            cleaned = cleaned.split('.')[-1]
        return cleaned

    # æ‰€æœ‰CREATE TABLEçš„è¡¨éƒ½ç®—ä¸´æ—¶è¡¨
    temp_tables = {clean_table_name(table) for table in create_matches}

    print(f"æ£€æµ‹åˆ°çš„ä¸´æ—¶è¡¨ï¼ˆæ‰€æœ‰CREATE TABLEï¼‰: {temp_tables}")
    return temp_tables


def split_sql_statements(sql_script):
    """
    ä½¿ç”¨sqllineageçš„splitæ–¹æ³•æ¥æ­£ç¡®æ‹†åˆ†SQLè¯­å¥
    """
    try:
        statements = split(sql_script)
        return [stmt.strip() for stmt in statements if stmt.strip()]
    except Exception as e:
        print(f"ä½¿ç”¨sqllineageæ‹†åˆ†SQLå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•æ‹†åˆ†: {e}")
        statements = []
        parts = sql_script.split(';')
        for part in parts:
            part = part.strip()
            if part and not part.isspace():
                statements.append(part)
        return statements


def is_temp_table(table_identifier, temp_tables):
    """
    æ£€æŸ¥è¡¨æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
    """
    if not table_identifier or not temp_tables:
        return False

    # æå–è¡¨åï¼ˆå¤„ç†å„ç§æ ¼å¼ï¼‰
    table_name = str(table_identifier).lower()
    if '.' in table_name:
        table_name = table_name.split('.')[-1]

    return table_name in temp_tables


def add_table_type_marker(table_name, is_temp_table, is_subquery_table):
    """
    ä¸ºè¡¨åæ·»åŠ ç±»å‹æ ‡è®°
    
    Args:
        table_name: åŸå§‹è¡¨å
        is_temp_table: æ˜¯å¦ä¸ºä¸´æ—¶è¡¨
        is_subquery_table: æ˜¯å¦ä¸ºå­æŸ¥è¯¢è¡¨
    
    Returns:
        str: å¸¦æ ‡è®°çš„è¡¨å
    """
    if not table_name:
        return table_name
    
    # å¦‚æœæ˜¯å­æŸ¥è¯¢è¡¨ï¼Œä¼˜å…ˆæ ‡è®°ä¸ºå­æŸ¥è¯¢ï¼ˆå› ä¸ºå­æŸ¥è¯¢è¡¨ä¹Ÿå¯èƒ½æ˜¯ä¸´æ—¶çš„ï¼‰
    if is_subquery_table:
        return f"{table_name}{TableTypeMarkers.SUBQUERY_TABLE_SUFFIX}"
    elif is_temp_table:
        return f"{table_name}{TableTypeMarkers.TEMP_TABLE_SUFFIX}"
    else:
        return table_name


def create_unique_subquery_table_name(subquery_alias, etl_job, sql_no):
    """
    ğŸ¯ åˆ›å»ºå”¯ä¸€çš„å­æŸ¥è¯¢è¡¨å
    æ ¼å¼ï¼š{å­æŸ¥è¯¢åˆ«å}_{ETLä½œä¸šåå“ˆå¸Œ}_{SQLåºå·}_SUBQRY_TBL
    
    Args:
        subquery_alias: å­æŸ¥è¯¢åˆ«å
        etl_job: ETLä½œä¸šåç§°
        sql_no: SQLåºå·
        
    Returns:
        str: å”¯ä¸€çš„å­æŸ¥è¯¢è¡¨å
    """
    import hashlib
    
    # ä¸ºäº†é¿å…è¡¨åè¿‡é•¿ï¼Œä½¿ç”¨ETLä½œä¸šåçš„MD5å“ˆå¸Œå€¼å‰8ä½
    etl_hash = ""
    if etl_job:
        etl_hash = hashlib.md5(etl_job.encode('utf-8')).hexdigest()[:8]
    
    sql_no_str = str(sql_no) if sql_no is not None else "0"
    
    # æ„å»ºå”¯ä¸€è¡¨åï¼šåˆ«å_å“ˆå¸Œ_SQLåºå·_æ ‡è®°
    unique_table_name = f"{subquery_alias}_{etl_hash}_{sql_no_str}{TableTypeMarkers.SUBQUERY_TABLE_SUFFIX}"
    
    return unique_table_name


def extract_database_table_column(column_id, temp_tables, subquery_nodes, current_database='', etl_job='', sql_no=None):
    """
    ä»å­—æ®µIDä¸­æå–æ•°æ®åº“ã€è¡¨ã€å­—æ®µä¿¡æ¯ï¼Œå¹¶ä¸ºè¡¨åæ·»åŠ ç±»å‹æ ‡è®°
    ğŸ¯ å¢å¼ºç‰ˆï¼šä¸ºå­æŸ¥è¯¢è¡¨åæ·»åŠ è„šæœ¬å’ŒSQLåºå·ä¿¡æ¯ï¼Œç¡®ä¿å”¯ä¸€æ€§
    
    Args:
        column_id: å­—æ®µæ ‡è¯†ç¬¦
        temp_tables: ä¸´æ—¶è¡¨é›†åˆ
        subquery_nodes: å­æŸ¥è¯¢èŠ‚ç‚¹é›†åˆ
        current_database: å½“å‰é»˜è®¤æ•°æ®åº“ï¼ˆæ¥è‡ªUSEè¯­å¥ï¼‰
        etl_job: ETLä½œä¸šåç§°ï¼ˆç”¨äºå­æŸ¥è¯¢å”¯ä¸€æ€§ï¼‰
        sql_no: SQLåºå·ï¼ˆç”¨äºå­æŸ¥è¯¢å”¯ä¸€æ€§ï¼‰
    """
    if not column_id:
        return None

    parts = str(column_id).split('.')
    
    # æå–åŸºæœ¬ä¿¡æ¯
    if len(parts) >= 3:
        # database.table.column æ ¼å¼
        database = parts[0] if parts[0] not in ('<unknown>', '<default>') else ''
        table = parts[1]
        column = parts[2]
    elif len(parts) == 2:
        # table.column æ ¼å¼ï¼ˆæ— æ•°æ®åº“å‰ç¼€ï¼‰
        database = ''
        table = parts[0]
        column = parts[1]
    elif len(parts) == 1:
        # åªæœ‰å­—æ®µå
        database = ''
        table = ''
        column = parts[0]
    else:
        return None
    
    # åˆ¤æ–­è¡¨ç±»å‹å¹¶æ·»åŠ æ ‡è®°
    if table:
        is_temp = is_temp_table(table, temp_tables)
        is_subquery = table in subquery_nodes
        
        if is_subquery:
            # ğŸ¯ å…³é”®æ”¹è¿›ï¼šä¸ºå­æŸ¥è¯¢è¡¨åæ·»åŠ å”¯ä¸€æ€§æ ‡è¯†
            table_with_marker = create_unique_subquery_table_name(table, etl_job, sql_no)
            database = '<SUBQUERY_DB>'
            print(f"ğŸ”§ ä¸ºå­æŸ¥è¯¢è¡¨ {table} åˆ›å»ºå”¯ä¸€æ ‡è¯†: {table_with_marker}")
        else:
            table_with_marker = add_table_type_marker(table, is_temp, False)
            if (not database or database == '<default>') and current_database:
                database = current_database
                print(f"ğŸ”§ ä¸ºè¡¨ {table} è¡¥å……é»˜è®¤æ•°æ®åº“: {current_database}")
    else:
        table_with_marker = table
    
    return {
        'database': database,
        'table': table_with_marker,
        'column': column
    }


def trace_lineage_through_subqueries(cytoscape_data, temp_tables, current_database=''):
    """
    åŸºäºsqllineageçš„SubQueryç±»å‹ä¿¡æ¯ï¼Œè¿½è¸ªè·¨å­æŸ¥è¯¢çš„è¡€ç¼˜å…³ç³»
    ä¿®æ”¹ï¼šä¸è¿‡æ»¤ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼Œè€Œæ˜¯ä¸ºå®ƒä»¬æ·»åŠ æ ‡è®°
    æ”¯æŒé»˜è®¤æ•°æ®åº“è¡¥å……
    """
    # æ„å»ºèŠ‚ç‚¹å’Œè¾¹çš„æ˜ å°„
    nodes_dict = {}
    edges = []
    subquery_nodes = set()
    
    for item in cytoscape_data:
        data = item.get("data", {})
        item_id = data.get("id", "")
        
        if "source" in data and "target" in data:
            edges.append(data)
        else:
            nodes_dict[item_id] = data
            # è¯†åˆ«SubQueryèŠ‚ç‚¹
            if data.get("type") == "SubQuery":
                subquery_nodes.add(item_id)
    
    # æ„å»ºå›¾ç»“æ„ï¼šå‡ºè¾¹å’Œå…¥è¾¹æ˜ å°„
    outgoing_edges = defaultdict(list)  # node_id -> [target_ids]
    incoming_edges = defaultdict(list)  # node_id -> [source_ids]
    
    for edge in edges:
        source_id = edge.get("source", "")
        target_id = edge.get("target", "")
        if source_id and target_id:
            outgoing_edges[source_id].append(target_id)
            incoming_edges[target_id].append(source_id)
    
    # æ”¶é›†æ‰€æœ‰è¡€ç¼˜è·¯å¾„ï¼ˆåŒ…æ‹¬ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼‰
    lineage_paths = []
    
    def is_subquery_column(column_id):
        """åˆ¤æ–­å­—æ®µæ˜¯å¦å±äºSubQuery"""
        if not column_id or '.' not in column_id:
            return False
        table_part = column_id.split('.')[0]
        return table_part in subquery_nodes
    
    def is_temp_table_column(column_id):
        """åˆ¤æ–­å­—æ®µæ˜¯å¦å±äºä¸´æ—¶è¡¨"""
        if not column_id or '.' not in column_id:
            return False
        table_part = column_id.split('.')[0]
        return is_temp_table(table_part, temp_tables)
    
    def is_real_table_column(column_id):
        """åˆ¤æ–­å­—æ®µæ˜¯å¦å±äºçœŸå®è¡¨ï¼ˆéSubQueryä¸”éä¸´æ—¶è¡¨ï¼‰"""
        if not column_id:
            return False
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯SubQueryå­—æ®µ
        if is_subquery_column(column_id):
            return False
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸´æ—¶è¡¨å­—æ®µ
        if is_temp_table_column(column_id):
            return False
            
        # æ£€æŸ¥è¯¥å­—æ®µçš„parent_candidates
        column_data = nodes_dict.get(column_id, {})
        parent_candidates = column_data.get("parent_candidates", [])
        
        for candidate in parent_candidates:
            if isinstance(candidate, dict):
                candidate_type = candidate.get("type", "")
                candidate_name = candidate.get("name", "")
                
                # å¦‚æœparentæ˜¯Tableç±»å‹ä¸”ä¸æ˜¯ä¸´æ—¶è¡¨ï¼Œåˆ™æ˜¯çœŸå®è¡¨å­—æ®µ
                if (candidate_type == "Table" and 
                    not is_temp_table(candidate_name, temp_tables)):
                    return True
        
        # å¦‚æœæ²¡æœ‰parent_candidatesï¼Œé€šè¿‡å­—æ®µIDåˆ¤æ–­
        if '.' in column_id:
            table_part = column_id.split('.')[0]
            return not is_temp_table(table_part, temp_tables)
        
        return False
    
    def trace_through_temp_tables(start_column_id, visited=None):
        """è¿½è¸ªè·¨è¶Šä¸´æ—¶è¡¨çš„è¡€ç¼˜å…³ç³»ï¼Œè¿”å›æœ€ç»ˆçš„çœŸå®è¡¨å­—æ®µ"""
        if visited is None:
            visited = set()
        
        if start_column_id in visited:
            return []  # é¿å…å¾ªç¯
        
        visited.add(start_column_id)
        
        # å¦‚æœå½“å‰å­—æ®µå°±æ˜¯çœŸå®è¡¨å­—æ®µï¼Œç›´æ¥è¿”å›
        if is_real_table_column(start_column_id):
            return [start_column_id]
        
        # å¦‚æœæ˜¯ä¸´æ—¶è¡¨å­—æ®µæˆ–SubQueryå­—æ®µï¼Œç»§ç»­è¿½è¸ªå…¶æºå­—æ®µ
        real_sources = []
        for source_id in incoming_edges.get(start_column_id, []):
            deeper_sources = trace_through_temp_tables(source_id, visited.copy())
            real_sources.extend(deeper_sources)
        
        return real_sources
    
    def trace_to_real_source(subquery_column_id, visited=None):
        """ä»å­æŸ¥è¯¢å­—æ®µè¿½è¸ªåˆ°çœŸå®æºè¡¨å­—æ®µ"""
        if visited is None:
            visited = set()
        
        if subquery_column_id in visited:
            return []  # é¿å…å¾ªç¯
        
        visited.add(subquery_column_id)
        real_sources = []
        
        # æŸ¥æ‰¾è¯¥å­æŸ¥è¯¢å­—æ®µçš„æ‰€æœ‰æºå­—æ®µ
        for source_id in incoming_edges.get(subquery_column_id, []):
            if is_subquery_column(source_id):
                # å¦‚æœæºè¿˜æ˜¯å­æŸ¥è¯¢å­—æ®µï¼Œç»§ç»­é€’å½’è¿½è¸ª
                deeper_sources = trace_to_real_source(source_id, visited.copy())
                real_sources.extend(deeper_sources)
            elif is_real_table_column(source_id):
                # å¦‚æœæºæ˜¯çœŸå®è¡¨å­—æ®µï¼Œæ·»åŠ åˆ°ç»“æœ
                real_sources.append(source_id)
            elif is_temp_table_column(source_id):
                # å¦‚æœæºæ˜¯ä¸´æ—¶è¡¨å­—æ®µï¼Œè¿½è¸ªåˆ°çœŸå®è¡¨
                deeper_sources = trace_through_temp_tables(source_id, visited.copy())
                real_sources.extend(deeper_sources)
        
        return real_sources
    
    # å¤„ç†ç­–ç•¥ï¼šæ”¶é›†æ‰€æœ‰è¡€ç¼˜å…³ç³»ï¼ŒåŒ…æ‹¬æ¶‰åŠä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨çš„
    
    if subquery_nodes:
        # æœ‰SubQueryçš„æƒ…å†µï¼šå¤„ç†è·¨SubQueryçš„è¡€ç¼˜å…³ç³»
        processed_subquery_columns = set()
        
        for edge in edges:
            source_id = edge.get("source", "")
            target_id = edge.get("target", "")
            
            # å¤„ç† SubQueryå­—æ®µ â†’ æœ€ç»ˆè¡¨å­—æ®µ çš„è¾¹
            if (is_subquery_column(source_id) and 
                not is_subquery_column(target_id) and  # ç›®æ ‡ä¸æ˜¯å­æŸ¥è¯¢å­—æ®µ
                source_id not in processed_subquery_columns):
                
                processed_subquery_columns.add(source_id)
                
                # è¿½è¸ªè¯¥SubQueryå­—æ®µçš„çœŸå®æºè¡¨
                real_sources = trace_to_real_source(source_id)
                
                # å»ºç«‹çœŸå®æºè¡¨åˆ°æœ€ç»ˆç›®æ ‡è¡¨çš„è¡€ç¼˜å…³ç³»
                for real_source_id in real_sources:
                    lineage_paths.append({
                        'source': real_source_id,
                        'target': target_id
                    })
    
    # å¤„ç†æ‰€æœ‰ç›´æ¥çš„å­—æ®µåˆ°å­—æ®µçš„è¾¹ï¼ˆåŒ…æ‹¬ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼‰
    for edge in edges:
        source_id = edge.get("source", "")
        target_id = edge.get("target", "")
        
        # æ”¶é›†æ‰€æœ‰ç›´æ¥çš„è¡€ç¼˜å…³ç³»ï¼ˆä¸å†è¿‡æ»¤ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼‰
        if source_id and target_id and '.' in source_id and '.' in target_id:
            lineage_paths.append({
                'source': source_id,
                'target': target_id
            })
        
    return lineage_paths, subquery_nodes


def process_cytoscape_lineage(cytoscape_data, temp_tables, current_database, etl_system, etl_job, sql_path, sql_no):
    """
    å¤„ç†cytoscapeæ ¼å¼çš„è¡€ç¼˜æ•°æ®
    ğŸ¯ å¢å¼ºç‰ˆï¼šæ”¯æŒå­æŸ¥è¯¢å”¯ä¸€æ€§æ ‡è¯†ï¼Œä¸è¿‡æ»¤ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼Œè€Œæ˜¯ä¸ºå®ƒä»¬æ·»åŠ æ ‡è®°
    æ”¯æŒé»˜è®¤æ•°æ®åº“è¡¥å……
    """
    lineage_records = []

    if not cytoscape_data:
        return lineage_records
    # ä½¿ç”¨åŸºäºsqllineage SubQueryç±»å‹çš„è¿½è¸ªç®—æ³•
    lineage_paths, subquery_nodes = trace_lineage_through_subqueries(cytoscape_data, temp_tables, current_database)
    
    for path in lineage_paths:
        source_id = path['source']
        target_id = path['target']
        
        # ğŸ¯ ä½¿ç”¨å¢å¼ºç‰ˆå‡½æ•°ï¼Œä¼ å…¥ETLä½œä¸šå’ŒSQLåºå·ä¿¡æ¯ä»¥ç¡®ä¿å­æŸ¥è¯¢å”¯ä¸€æ€§
        source_info = extract_database_table_column(source_id, temp_tables, subquery_nodes, current_database, etl_job, sql_no)
        if not source_info or not source_info['table']:
            continue
        
        # ğŸ¯ ä½¿ç”¨å¢å¼ºç‰ˆå‡½æ•°ï¼Œä¼ å…¥ETLä½œä¸šå’ŒSQLåºå·ä¿¡æ¯ä»¥ç¡®ä¿å­æŸ¥è¯¢å”¯ä¸€æ€§  
        target_info = extract_database_table_column(target_id, temp_tables, subquery_nodes, current_database, etl_job, sql_no)
        if not target_info or not target_info['table']:
            continue
        
        # æ·»åŠ è¡€ç¼˜è®°å½•ï¼ˆè¡¨åå·²åŒ…å«å”¯ä¸€æ€§æ ‡è¯†ï¼‰
        record = {
            'etl_system': etl_system,
            'etl_job': etl_job,
            'sql_path': sql_path,
            'sql_no': sql_no,
            'source_database': source_info['database'],
            'source_table': source_info['table'],  # å­æŸ¥è¯¢è¡¨å·²åŒ…å«å”¯ä¸€æ€§æ ‡è¯†
            'source_column': source_info['column'],
            'target_database': target_info['database'],
            'target_table': target_info['table'],  # å­æŸ¥è¯¢è¡¨å·²åŒ…å«å”¯ä¸€æ€§æ ‡è¯†
            'target_column': target_info['column']
        }
        
        lineage_records.append(record)
    
    print(f"âœ… è§£æå‡º {len(lineage_records)} æ¡å­—æ®µçº§è¡€ç¼˜å…³ç³»ï¼ˆå­æŸ¥è¯¢è¡¨åå·²æ·»åŠ å”¯ä¸€æ€§æ ‡è¯†ï¼‰")
    return lineage_records


# DDLå’Œæ§åˆ¶è¯­å¥ç±»å‹å¸¸é‡
class DDLStatementTypes:
    """ä¸éœ€è¦è§£æè¡€ç¼˜å…³ç³»çš„è¯­å¥ç±»å‹ï¼ˆç±»ä¼¼Javaé™æ€ç±»/æšä¸¾ï¼‰"""
    
    # ä¸éœ€è¦è§£æè¡€ç¼˜å…³ç³»çš„è¯­å¥å…³é”®å­—ï¼ˆæ³¨æ„ï¼šUSEè¯­å¥å·²ç§»é™¤ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
    SKIP_KEYWORDS = frozenset([
        'ALTER', 'DROP', 'GRANT', 'REVOKE', 'SET', 'SHOW', 
        'DESCRIBE', 'DESC', 'EXPLAIN', 'ANALYZE', 'TRUNCATE', 
        'COMMENT', 'REFRESH', 'MSCK', 'CACHE', 'UNCACHE',
        'CREATE DATABASE', 'CREATE SCHEMA', 'CREATE USER', 'CREATE ROLE', 
        'CREATE INDEX', 'CREATE FUNCTION', 'CREATE PROCEDURE'
    ])


def is_ddl_or_control_statement(sql_statement):
    """
    æ£€æµ‹SQLè¯­å¥æ˜¯å¦ä¸ºä¸éœ€è¦è§£æè¡€ç¼˜å…³ç³»çš„è¯­å¥
    ä¿®æ”¹ï¼šUSEè¯­å¥ä¸è·³è¿‡ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†æ¥æå–æ•°æ®åº“å
    """
    if not sql_statement or not sql_statement.strip():
        return False, None
    
    # ç®€å•åˆ†è¯å¤„ç†ï¼ˆå‡è®¾è¾“å…¥å·²æ¸…ç†è¿‡æ³¨é‡Šï¼‰
    sql_upper = sql_statement.strip().upper()
    words = sql_upper.split()
    
    if not words:
        return False, None
    
    # ç‰¹æ®Šå¤„ç†USEè¯­å¥ - ä¸è·³è¿‡ï¼Œéœ€è¦æå–æ•°æ®åº“å
    if words[0] == 'USE':
        return False, None  # ä¸è·³è¿‡USEè¯­å¥ï¼Œè®©åç»­é€»è¾‘å¤„ç†
    
    # æ£€æŸ¥å•å…³é”®è¯è¯­å¥
    first_word = words[0]
    if first_word in DDLStatementTypes.SKIP_KEYWORDS:
        return True, first_word
    
    # æ£€æŸ¥ä¸¤å…³é”®è¯ç»„åˆè¯­å¥
    if len(words) >= 2:
        two_words = f"{words[0]} {words[1]}"
        if two_words in DDLStatementTypes.SKIP_KEYWORDS:
            return True, two_words
    
    # ç‰¹æ®Šå¤„ç†CREATEè¯­å¥
    if first_word == 'CREATE' and len(words) >= 2:
        second_word = words[1]
        
        # CREATE TABLE/VIEW è¯­å¥ç»†åˆ†
        if second_word in ('TABLE', 'VIEW') or (second_word in ('TEMPORARY', 'TEMP') and len(words) >= 3 and words[2] in ('TABLE', 'VIEW')):
            # å¦‚æœåŒ…å«ASå…³é”®å­—ï¼Œè¯´æ˜æ˜¯CREATE TABLE AS SELECTï¼Œæœ‰è¡€ç¼˜å…³ç³»ï¼Œéœ€è¦è§£æ
            if 'AS' in words and 'SELECT' in words:
                return False, None  # ä¸è·³è¿‡ï¼Œéœ€è¦è§£æè¡€ç¼˜å…³ç³»
            else:
                # çº¯CREATE TABLEå®šä¹‰è¯­å¥ï¼Œæ— è¡€ç¼˜å…³ç³»ï¼Œè·³è¿‡
                if second_word in ('TEMPORARY', 'TEMP'):
                    return True, f'CREATE {second_word} {words[2]}'
                else:
                    return True, f'CREATE {second_word}'
    
    # å…¶ä»–è¯­å¥ï¼ˆINSERTã€SELECTç­‰ï¼‰éƒ½ä¸è·³è¿‡ï¼Œæ­£å¸¸è§£æè¡€ç¼˜å…³ç³»
    return False, None


def is_from_statement(sql_statement):
    """
    æ£€æµ‹SQLè¯­å¥æ˜¯å¦ä»¥FROMå¼€å¤´ï¼ˆHiveç‰¹æ®Šè¯­æ³•ï¼‰
    æ”¯æŒå¤„ç†FROM(è¿™ç§è¿å†™çš„æƒ…å†µ
    
    Args:
        sql_statement: SQLè¯­å¥
        
    Returns:
        bool: æ˜¯å¦ä¸ºFROMå¼€å¤´çš„è¯­å¥
    """
    if not sql_statement or not sql_statement.strip():
        return False
    
    # å»é™¤å‰å¯¼ç©ºç™½å¹¶è½¬æ¢ä¸ºå¤§å†™
    sql_upper = sql_statement.strip().upper()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…FROMå¼€å¤´ï¼ŒåŒ…æ‹¬FROM(çš„æƒ…å†µ
    from_pattern = r'^\s*FROM\s*(?:\(|\s)'
    return bool(re.match(from_pattern, sql_upper))


def process_single_sql(sql_statement, temp_tables, current_database, etl_system, etl_job, sql_path, sql_no, db_type='oracle'):
    """
    å¤„ç†å•æ¡SQLè¯­å¥ï¼Œè·å–è¡€ç¼˜å…³ç³»
    ä¿®æ”¹ï¼šæ”¯æŒUSEè¯­å¥å¤„ç†ã€é»˜è®¤æ•°æ®åº“ç»´æŠ¤å’ŒFROMå¼€å¤´è¯­å¥çš„non-validatingå¤„ç†
    
    Returns:
        tuple: (lineage_records, new_current_database)
    """
    lineage_records = []
    new_current_database = current_database
    
    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºUSEè¯­å¥
    use_database = extract_use_database(sql_statement)
    if use_database:
        new_current_database = use_database
        return lineage_records, new_current_database
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºDDLæˆ–æ§åˆ¶è¯­å¥
    is_ddl, stmt_type = is_ddl_or_control_statement(sql_statement)
    
    if is_ddl:
        print(f"â­ï¸  è·³è¿‡{stmt_type}è¯­å¥ï¼ˆæ— è¡€ç¼˜å…³ç³»è§£ææ„ä¹‰ï¼‰")
        return lineage_records, new_current_database
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºFROMå¼€å¤´çš„è¯­å¥ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨non-validating dialect
    actual_db_type = db_type
    if is_from_statement(sql_statement):
        actual_db_type = 'non-validating'
        print(f"ğŸ”§ æ£€æµ‹åˆ°FROMå¼€å¤´è¯­å¥ï¼Œä½¿ç”¨non-validating dialectè§£æ")
    
    try:
        # ä½¿ç”¨LineageRunneråˆ†æSQLï¼Œæ ¹æ®è¯­å¥ç±»å‹é€‰æ‹©é€‚å½“çš„dialect
        runner = LineageRunner(sql_statement, dialect=actual_db_type, silent_mode=True)

        # è·å–cytoscapeæ ¼å¼çš„å­—æ®µçº§è¡€ç¼˜æ•°æ®
        try:
            cytoscape_data = runner.to_cytoscape(LineageLevel.COLUMN)
            
            if cytoscape_data and isinstance(cytoscape_data, list):
                lineage_records = process_cytoscape_lineage(cytoscape_data, temp_tables, current_database, etl_system, etl_job, sql_path, sql_no)
                print(f"âœ… è§£æå‡º {len(lineage_records)} æ¡å­—æ®µçº§è¡€ç¼˜å…³ç³»")
            else:
                print(f"âŒ {sql_path} æœªè·å–åˆ°å­—æ®µçº§è¡€ç¼˜æ•°æ®")

        except Exception as e:
            print(f"âŒ  {sql_path} è·å–å­—æ®µçº§è¡€ç¼˜å¤±è´¥: {e}")

    except Exception as e:
        print(f"âŒ  {sql_path} åˆ›å»ºLineageRunneræ—¶å‡ºé”™: {e}")

    return lineage_records, new_current_database


def generate_oracle_insert_statements(lineage_records, etl_system, etl_job):
    """
    ç”ŸæˆOracle INSERTè¯­å¥ï¼ˆåŒ…å«etl_systemã€etl_jobã€sql_pathã€sql_noå­—æ®µï¼‰
    ä¼˜åŒ–ç‰ˆï¼šç›´æ¥ä¼ å…¥etl_systemå’Œetl_jobå‚æ•°ï¼Œå…ˆåˆ é™¤å†æ‰¹é‡æ’å…¥ï¼Œæœ€åæäº¤äº‹åŠ¡
    
    Args:
        lineage_records: è¡€ç¼˜å…³ç³»è®°å½•åˆ—è¡¨
        etl_system: ETLç³»ç»Ÿåç§°
        etl_job: ETLä½œä¸šåç§°
        
    Returns:
        str: Oracle DELETEå’ŒINSERTè¯­å¥
    """
    insert_statements = []


    
    # ç”ŸæˆDELETEè¯­å¥
    insert_statements.append("-- ç¬¬ä¸€æ­¥ï¼šåˆ é™¤ç°æœ‰æ•°æ®ï¼ˆåŸºäºETL_SYSTEMå’ŒETL_JOBï¼‰")

    # etl_system å’Œ etl_job éƒ½ä¸ºç©ºæ—¶ï¼Œä¸åˆ é™¤æ•°æ®  é¿å…è¯¯åˆ æ•°æ®
    if etl_system != '' and etl_job != '' and etl_system != None and etl_job != None:
        delete_sql = f"DELETE FROM LINEAGE_TABLE WHERE ETL_SYSTEM = '{etl_system}' AND ETL_JOB = '{etl_job}';"
        insert_statements.append(delete_sql)
        #TODO æ‰§è¡Œæ•°æ®åº“åˆ é™¤æ“ä½œ
        
    insert_statements.append("")
    
    insert_statements.append("-- ç¬¬äºŒæ­¥ï¼šæ‰¹é‡æ’å…¥æ–°æ•°æ®")

    # ç”Ÿæˆæ‰€æœ‰INSERTè¯­å¥
    for record in lineage_records:
        def format_value(value):
            if not value or value == '':
                return 'NULL'
            else:
                escaped_value = str(value).replace("'", "''")
                return f"'{escaped_value}'"

        etl_system_val = format_value(record['etl_system'])
        etl_job_val = format_value(record['etl_job'])
        sql_path = format_value(record['sql_path'])
        sql_no = record['sql_no'] if record['sql_no'] is not None else 'NULL'
        source_db = format_value(record['source_database'])
        source_table = format_value(record['source_table'])
        source_column = format_value(record['source_column'])
        target_db = format_value(record['target_database'])
        target_table = format_value(record['target_table'])
        target_column = format_value(record['target_column'])

        insert_sql = f"""INSERT INTO LINEAGE_TABLE (ETL_SYSTEM, ETL_JOB, SQL_PATH, SQL_NO, SOURCE_DATABASE, SOURCE_TABLE, SOURCE_COLUMN, TARGET_DATABASE, TARGET_TABLE, TARGET_COLUMN)
VALUES ('{etl_system_val}', '{etl_job_val}', '{sql_path}', '{sql_no}', '{source_db}', '{source_table}', '{source_column}', '{target_db}', '{target_table}', '{target_column}');"""

        insert_statements.append(insert_sql)

    insert_statements.append("")
    insert_statements.append("-- ç¬¬ä¸‰æ­¥ï¼šæäº¤äº‹åŠ¡")
    insert_statements.append("COMMIT;")
    #TODO å°†è„šæœ¬å†…æ‰€æœ‰sqlæ‹¼æ¥å¥½ä¹‹åï¼ŒæŒ‰ç…§è„šæœ¬ç»´åº¦æ‰§è¡Œ é¿å…é¢‘ç¹æäº¤äº‹åŠ¡

    return "\n".join(insert_statements)


def process_sql_script(sql_script, etl_system='', etl_job='', sql_path='', db_type=''):
    """
    å¤„ç†SQLè„šæœ¬ï¼ˆæ”¯æŒå•æ¡SQLæˆ–å®Œæ•´è„šæœ¬ï¼‰
    ä¿®æ”¹ï¼šæ”¯æŒUSEè¯­å¥å¤„ç†å’Œé»˜è®¤æ•°æ®åº“ç»´æŠ¤ï¼Œå…ˆåˆ é™¤å†æ’å…¥æ¨¡å¼
    """
    # 1. æå–ä¸´æ—¶è¡¨
    temp_tables = extract_temp_tables_from_script(sql_script)

    # 2. æ‹†åˆ†SQLè¯­å¥
    sql_statements = split_sql_statements(sql_script)
    print(f"å…±æ‰¾åˆ° {len(sql_statements)} æ¡SQLè¯­å¥")

    # 3. å¤„ç†æ¯æ¡SQLï¼Œç»´æŠ¤å½“å‰æ•°æ®åº“çŠ¶æ€
    all_lineage_records = []
    current_database = ''  # é»˜è®¤æ•°æ®åº“å˜é‡
    
    for i, sql in enumerate(sql_statements):
        sql_no = i + 1
        print(f"å¤„ç†ç¬¬ {sql_no}/{len(sql_statements)} æ¡SQL...")
        
        # å¤„ç†SQLè¯­å¥ï¼Œè¿”å›è¡€ç¼˜è®°å½•å’Œæ›´æ–°åçš„å½“å‰æ•°æ®åº“
        lineage_records, current_database = process_single_sql(
            sql, temp_tables, current_database, etl_system, etl_job, sql_path, sql_no, db_type
        )
        all_lineage_records.extend(lineage_records)
        
        print(f"  æ–°å¢ {len(lineage_records)} æ¡è¡€ç¼˜å…³ç³»")

    print(f"å…±æå–åˆ° {len(all_lineage_records)} æ¡è¡€ç¼˜å…³ç³»")

    # 4. ç”ŸæˆOracle DELETEå’ŒINSERTè¯­å¥
    oracle_statements = generate_oracle_insert_statements(all_lineage_records, etl_system, etl_job)

    return oracle_statements


def lineage_analysis(sql=None, file=None, db_type='oracle'):
    """
    è¡€ç¼˜å…³ç³»åˆ†æä¸»å…¥å£ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼šå…ˆåˆ é™¤å†æ’å…¥æ¨¡å¼ï¼‰
    
    Args:
        sql: SQLè„šæœ¬å†…å®¹å­—ç¬¦ä¸²
        file: SQLæ–‡ä»¶è·¯å¾„ï¼ˆå•ä¸ªæ–‡ä»¶æˆ–ç›®å½•ï¼‰
        db_type: æ•°æ®åº“ç±»å‹ï¼Œé»˜è®¤'oracle'
        
    Returns:
        str: Oracle DELETEå’ŒINSERTè¯­å¥ï¼ˆåŒ…å«æ ‡è®°çš„ä¸´æ—¶è¡¨å’Œå­æŸ¥è¯¢è¡¨ï¼Œæ”¯æŒé»˜è®¤æ•°æ®åº“ï¼‰
    """
    
    if sql is not None and file is not None:
        raise ValueError("sqlå’Œfileå‚æ•°ä¸èƒ½åŒæ—¶æä¾›ï¼Œåªèƒ½é€‰æ‹©å…¶ä¸­ä¸€ä¸ª")
    
    if sql is None and file is None:
        raise ValueError("å¿…é¡»æä¾›sqlæˆ–fileå‚æ•°")

    if sql is not None:
        # å¤„ç†SQLå­—ç¬¦ä¸²
        print("=== å¤„ç†SQLå­—ç¬¦ä¸²ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼šå…ˆåˆ é™¤å†æ’å…¥æ¨¡å¼ï¼‰===")
        return process_sql_script(sql, etl_system='DEMO_SYSTEM', etl_job='DEMO_JOB', sql_path='INLINE_SQL', db_type=db_type)
        
    elif file is not None:
        # å¤„ç†æ–‡ä»¶è·¯å¾„
        
        if os.path.isfile(file):
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    sql_content = f.read()
                
                # æ™ºèƒ½é€‰æ‹©base_pathï¼šå‘ä¸Šæ‰¾åˆ°åˆé€‚çš„åŸºç¡€ç›®å½•
                # ç­–ç•¥ï¼šå¦‚æœçˆ¶ç›®å½•çš„åç§°çœ‹èµ·æ¥åƒç³»ç»Ÿåï¼ˆåŒ…å«-æˆ–_ï¼‰ï¼Œåˆ™ä½¿ç”¨çˆ·çˆ·ç›®å½•ä½œä¸ºbase_path
                file_dir = os.path.dirname(file)
                parent_name = os.path.basename(file_dir)
                
                # å¦‚æœå½“å‰ç›®å½•ååŒ…å«å…¸å‹çš„ç³»ç»Ÿæ ‡è¯†ç¬¦ï¼Œä½¿ç”¨ä¸Šçº§ç›®å½•ä½œä¸ºbase_path
                if '-' in parent_name or '_' in parent_name:
                    base_path = os.path.dirname(file_dir)
                else:
                    base_path = file_dir
                
                etl_info = parse_etl_info_from_path(file, base_path)
                
                return process_sql_script(sql_content, etl_info['etl_system'], etl_info['etl_job'], file, db_type)
                
            except Exception as e:
                return f"-- å¤„ç†æ–‡ä»¶å¤±è´¥: {e}"
                
        elif os.path.isdir(file):
            # å¤„ç†ç›®å½• - ä½¿ç”¨æ–°çš„è·¯å¾„è§£æé€»è¾‘
            sql_extensions = ['*.sql', '*.SQL', '*.hql', '*.HQL']
            
            # ä½¿ç”¨é›†åˆå»é‡ï¼Œé¿å…Windowsç³»ç»Ÿä¸­å¤§å°å†™ä¸æ•æ„Ÿå¯¼è‡´çš„é‡å¤æ–‡ä»¶
            all_files = set()
            for ext in sql_extensions:
                pattern = os.path.join(file, '**', ext)
                files = glob.glob(pattern, recursive=True)
                all_files.update(files)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åºï¼Œç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
            sql_files = sorted(list(all_files))
            file_count = len(sql_files)
            
            if file_count == 0:
                return "-- æœªæ‰¾åˆ°ä»»ä½•SQLæ–‡ä»¶"
            
            print(f"æ‰¾åˆ° {file_count} ä¸ªSQLæ–‡ä»¶")
            
            all_results = []
            
            for i, sql_file in enumerate(sql_files):
                try:
                    print(f"\nå¤„ç†æ–‡ä»¶ {i+1}/{file_count}: {sql_file}")
                    
                    with open(sql_file, 'r', encoding='utf-8') as f:
                        sql_content = f.read()
                    
                    # ä»å®Œæ•´è·¯å¾„ä¸­è§£æETLä¿¡æ¯
                    etl_info = parse_etl_info_from_path(sql_file, file)
                    print(f"  è§£æåˆ°çš„ETLä¿¡æ¯: etl_system={etl_info['etl_system']}, etl_job={etl_info['etl_job']}, appname={etl_info['appname']}")
                    
                    result = process_sql_script(sql_content, etl_info['etl_system'], etl_info['etl_job'], sql_file, db_type)
                    all_results.append(result)
                    
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶ {sql_file} å¤±è´¥: {e}")
                    all_results.append(f"-- æ–‡ä»¶: {sql_file}\n-- å¤„ç†å¤±è´¥: {e}")
            
            # åˆå¹¶ç»“æœ
            combined_result = []
            combined_result.append(f"-- å…±å¤„ç† {file_count} ä¸ªæ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼šå…ˆåˆ é™¤å†æ’å…¥ï¼Œæ”¯æŒETLè·¯å¾„è§£æå’ŒUSEè¯­å¥çš„é»˜è®¤æ•°æ®åº“ï¼‰")
            combined_result.append("")
            
            for result in all_results:
                combined_result.append(result)
                combined_result.append("")
            
            return "\n".join(combined_result)
        else:
            return f"-- è·¯å¾„ä¸å­˜åœ¨: {file}"


if __name__ == "__main__":
    
    # æµ‹è¯•SQLç¤ºä¾‹ï¼ˆåŒ…å«USEè¯­å¥ï¼‰
    test_sql = """
    

    """
    
    result = lineage_analysis(sql=test_sql, db_type='oracle')
    print("ç»“æœ:")
    print(result) 